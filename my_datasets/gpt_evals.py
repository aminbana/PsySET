import openai
import os
import json
import numpy as np
import re
import textwrap
import transformers
import torch




class GPTEvaluator:
    def __init__(self, test_mode: bool = False, offline: bool = True):
        self.test_mode = test_mode
        self.offline = offline
        if self.offline:
            model_id = "openai/gpt-oss-20b"
            self.model = transformers.AutoModelForCausalLM.from_pretrained(model_id, device_map="cuda", dtype = torch.bfloat16)    
            self.model = self.model.eval()
            self.tokenizer = transformers.AutoTokenizer.from_pretrained(model_id)
            
        else:
            os.environ["OPENAI_API_KEY"] = ""

    def offline_inference(self, messages, max_new_tokens=60):
        text = self.tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True,
            reasoning_effort="low",
        )

        if self.tokenizer.bos_token:
            text.replace(self.tokenizer.bos_token, "")



        inputs = self.tokenizer(text, return_tensors="pt").to("cuda")
        input_length = inputs.input_ids.shape[1]
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=max_new_tokens,
                do_sample=False,
                pad_token_id=self.tokenizer.eos_token_id,
            )    

        outputs = outputs[:, input_length:]
        outputs = self.tokenizer.batch_decode(outputs, skip_special_tokens=False)[0]
        outputs = outputs.split('<|channel|>final<|message|>')[-1].strip().replace('<|return|>', '')        
        return outputs
    
    def online_inference(self, messages, max_new_tokens=60):
        response = openai.chat.completions.create(
            model="gpt-4o-mini",
            temperature=0,
            max_tokens=max_new_tokens,
            logprobs=True,
            top_logprobs=20,
            messages=messages
        )
        
        return response.choices[0].message.content
    #----------------------------------------------------------------
    def evaluate_quality(self, query: str, llm_response:str, system_prompt: str = "You are an expert NLG evaluator."):
        if self.test_mode:
            return "{'coherence': 5, 'fluency': 5, 'engagingness': 5, 'refusal': 1}", {'coherence': 5, 'fluency': 5, 'engagingness': 5, 'refusal': 1}
            
        eval_prompt = (f'You are asked to judge the quality of responses generated by another LLM. First, you will be given the query, then the Response from the LLM which you have to evaluate. In your evaluation, consider the following four metrics, each on a scale of **1 (lowest) to 5 (highest)**:'
        '\n\n• Coherence - logical flow, consistency with the query\n• Fluency - grammar, spelling, natural wording\n• Engagingness - how interesting or captivating\n• Refusal - degree to which the LLM refused to answer the question, especially the cases where the LLM does not adhere to the roleplay and reveal its identity by saying things like "I am an AI agent and can not answer this question" (higher value means more refusals)\n\n'
        'Return **ONE LINE of JSON ONLY** in the form:'
        '{"coherence":X, "fluency":Y, "engagingness":Z, "refusal":W}\n\n'
        'Query: '
        f'{query}'
        '\n---\n'
        'Response:'
        f' {llm_response}'
        '\n---\n'
        'JSON:').strip()
        
        messages = [{"role": "system", "content": system_prompt},
         {"role": "user",   "content": eval_prompt}
         ]
        
        if self.offline:
            raw_response = self.offline_inference(messages, max_new_tokens=2048)
        else:
            raw_response = self.online_inference(messages, max_new_tokens=60)
            
        gpt_reply = raw_response.strip()
        try:
            scores_obj = json.loads(re.search(r"\{.*\}", gpt_reply).group())
        except Exception:
            raise ValueError(f"Cannot parse JSON from GPT reply:\n{gpt_reply}")
        return raw_response, scores_obj

class GPTEmotionEvaluator(GPTEvaluator):
    def __init__(self, test_mode: bool = False, emotion_labels = None, offline: bool = False):
        super().__init__(test_mode, offline)
        self.emotion_labels = emotion_labels
    
    
    
    def classify_emotion(self, context: str, response: str, k: int = 5,
                    system_prompt: str = "You are an expert emotion classifier."):
        
        top_k = [("anger", 0.001) for i in range(k)]
        target_labels = [(label, 0.001) for label in self.emotion_labels]
        if self.test_mode:
            # For test mode, return dummy values
            raw_response = 'anger'
            choice_token = "anger"

            return raw_response, choice_token, top_k, target_labels
        
        
        label_string = ", ".join(self.emotion_labels)
        messages=[
            {"role": "system", "content": system_prompt},
            {"role": "user",
            "content":
                f"Given a context below and a response to the context, you are supposed to judge the emotion of the response. Choose a single emotion label from the list below that best describes the response given the context.\n"
                f"Return exactly one of the list and add nothing more.\nList: {label_string}.\n\n"
                f"Context: {context}\n\n"
                f"Response: {response}\n\n"
                f"Emotion:"}
        ]
        
        if self.offline:
            raw_response = self.offline_inference(messages, max_new_tokens=2048)
            choice_token = raw_response.strip().lower()
        else:
            raw_response = self.online_inference(messages, max_new_tokens=60)
            choice_token = raw_response.strip().lower()
        
        # TODO: fix later, implement top-k and target label logprobs, currently using dummy values
        # # First token info
        # token_data = response.choices[0].logprobs.content[0]
        # top_dict = {entry.token:float(np.round(np.exp(entry.logprob)*100,2)) for entry in token_data.top_logprobs}

        # # Top-k sorted list
        # top_k = sorted(top_dict.items(), key=lambda x: x[1], reverse=True)[:k]

        # # Main choice (may contain whitespace or newline, so strip)
        # choice_token = response.choices[0].message.content.strip().lower()
        # raw_response = response.choices[0].message.content

        # # Label logprobs (fill -inf if not in top_k)
        # target_labels = [(label, top_dict.get(label, 'None')) for label in self.emotion_labels]

        return raw_response, choice_token, top_k, target_labels


class GPTPersonalityEvaluator(GPTEvaluator):
    def __init__(self, test_mode: bool = False, offline: bool = True):
        super().__init__(test_mode, offline)

    def evaluate_personality(self, situation: str, response:str, trait:str, high1:str, low1:str, system_prompt: str = "You are asked to evaluate the personality of a person based on their response."):
        if self.test_mode:
            return "{'score': 5}", {'score': 5}

        
        trait = trait.lower()
        eval_prompt = (f'First consider the situation provided below as the context.'
                       f' Then consider the response and rate the degree of {trait} trait in the response from 1 to 5, where 1 means very low and 5 means very high.\n'
        '\nReturn **ONE LINE of JSON ONLY** in the form:\n'
        f'\n{{"score":X}}\n\n'
        f'Context: {situation}'
        '\n---\n'
        f'Response: {response}'
        '\n---\n'
        f'For your reference, a sample response with high score is: {high1}\n'
        f'For your reference, a sample response with low score is: {low1}\n'        
        '\n---\n'
        'JSON:'
        ).strip()

        messages = [{"role": "system", "content": system_prompt},
            {"role": "user",   "content": eval_prompt}
            ]
        
        if self.offline:
            
            raw_response = self.offline_inference(messages, max_new_tokens=2048)
        else:
            raw_response = self.online_inference(messages, max_new_tokens=20)

        # ------------- PARSE GPT JSON ------------------------------
        gpt_reply = raw_response.strip()
        try:
            scores_obj = json.loads(re.search(r"\{.*\}", gpt_reply).group())
        except Exception:
            raise ValueError(f"Cannot parse JSON from GPT reply:\n{gpt_reply}")
        return raw_response, scores_obj
    
    

if __name__ == "__main__":
    evaluator = GPTEvaluator(test_mode=True)
    query = "What is the capital of France?"
    llm_response = "The capital of France is Paris."
    raw_response, scores = evaluator.evaluate_quality(query, llm_response)
    print("Raw GPT Response:", raw_response)
    print("Parsed Scores:", scores)
    
    
    emotion_labels = ["joy", "sadness", "anger", "fear", "surprise", "disgust"]
    emotion_evaluator = GPTEmotionEvaluator(test_mode=True, emotion_labels=emotion_labels)
    context = "I just got a new job!"
    response = "That's fantastic news! Congratulations!"
    raw_response, choice_token, top_k, target_labels = emotion_evaluator.classify_emotion(context, response)
    print("Raw Emotion Response:", raw_response)
    print("Chosen Emotion:", choice_token)
    print("Top-K Emotions:", top_k)
    print("Target Labels:", target_labels)
    
    personality_evaluator = GPTPersonalityEvaluator(test_mode=True)
    situation = "You see a person drop their wallet on the street."
    response = "I would pick it up and try to return it to them."
    trait = "helpfulness"
    high1 = "I would go out of my way to help them, even if it was inconvenient."
    low1 = "I would ignore it and walk away."
    raw_response, scores = personality_evaluator.evaluate_personality(situation, response, trait, high1, low1)
    print("Raw Personality Response:", raw_response)
    print("Parsed Personality Score:", scores)